# import streamlit as st
# import os
# import time
# import random
# import sqlite3
# from datetime import datetime
# from dotenv import load_dotenv
# from openai import OpenAI, RateLimitError
# import tiktoken
# from fpdf import FPDF
# import base64
# import matplotlib.pyplot as plt
# from PyPDF2 import PdfReader
# from docx import Document as DocxDocument

# # ===== –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î =====
# def init_db():
#     conn = sqlite3.connect("history.db")
#     conn.execute("""
#         CREATE TABLE IF NOT EXISTS checks (
#             id INTEGER PRIMARY KEY AUTOINCREMENT,
#             check_type TEXT,
#             filename TEXT,
#             report TEXT,
#             similarity TEXT,
#             timestamp TEXT,
#             pdf_path TEXT
#         )
#     """)
#     conn.commit()
#     return conn

# def save_to_db(check_type, filename, report, similarity_list, pdf_path):
#     conn = init_db()
#     similarity_str = ",".join(map(str, similarity_list)) if isinstance(similarity_list, list) else str(similarity_list)
#     conn.execute(
#         "INSERT INTO checks (check_type, filename, report, similarity, timestamp, pdf_path) VALUES (?, ?, ?, ?, ?, ?)",
#         (check_type, filename, report, similarity_str, datetime.now().strftime("%Y-%m-%d %H:%M"), pdf_path)
#     )
#     conn.commit()
#     conn.close()

# def load_history():
#     conn = init_db()
#     rows = conn.execute("SELECT * FROM checks ORDER BY id DESC").fetchall()
#     conn.close()
#     return rows

# # ===== API –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è =====
# load_dotenv()
# OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# if not OPENAI_API_KEY:
#     st.error("‚ùå API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")
#     st.stop()
# client = OpenAI(api_key=OPENAI_API_KEY)

# # ===== –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ =====
# def extract_text(file) -> str:
#     """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–æ–≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
#     try:
#         if file.type == "application/pdf":
#             return "".join(page.extract_text() or "" for page in PdfReader(file).pages)
#         elif file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
#             doc = DocxDocument(file)
#             return "\n".join(p.text for p in doc.paragraphs)
#         elif file.type == "text/plain":
#             return file.read().decode("utf-8")
#     except Exception as e:
#         st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {str(e)}")
#         return ""
#     return ""

# def count_tokens(text: str, model="gpt-4") -> int:
#     """–ü–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
#     try:
#         encoding = tiktoken.encoding_for_model(model)
#         return len(encoding.encode(text))
#     except:
#         return len(text.split())  # Fallback –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

# def split_text_into_chunks(text, max_tokens=2000, model="gpt-4"):
#     """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç–æ–∫–µ–Ω–æ–≤"""
#     encoding = tiktoken.encoding_for_model(model)
#     tokens = encoding.encode(text)
#     chunks = []
    
#     for i in range(0, len(tokens), max_tokens):
#         chunk_tokens = tokens[i:i+max_tokens]
#         chunks.append(encoding.decode(chunk_tokens))
    
#     return chunks

# def safe_chat_completion(messages, model="gpt-4", max_tokens=1000, retries=3):
#     """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ OpenAI —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π"""
#     for attempt in range(retries):
#         try:
#             return client.chat.completions.create(
#                 model=model,
#                 messages=messages,
#                 max_tokens=max_tokens,
#                 temperature=0.7
#             )
#         except RateLimitError:
#             wait_time = 20 * (attempt + 1)
#             st.warning(f"‚è≥ –õ–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤. –ñ–¥—ë–º {wait_time} —Å–µ–∫...")
#             time.sleep(wait_time)
#         except Exception as e:
#             st.error(f"–û—à–∏–±–∫–∞ API: {str(e)}")
#             break
#     return None

# def generate_pdf(report: str, file_name="report.pdf"):
#     """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è PDF —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Unicode"""
#     pdf = FPDF()
#     pdf.add_page()
    
#     # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ Unicode (–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —à—Ä–∏—Ñ—Ç—ã)
#     font_added = False
#     font_paths = [
#         "DejaVuSansCondensed.ttf",
#         "arial.ttf",
#         "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",
#         "C:/Windows/Fonts/arial.ttf"
#     ]
    
#     for font_path in font_paths:
#         try:
#             pdf.add_font('CustomFont', '', font_path, uni=True)
#             pdf.set_font('CustomFont', '', 12)
#             font_added = True
#             break
#         except:
#             continue
    
#     if not font_added:
#         pdf.set_font("Arial", size=12)  # –ü–æ—Å–ª–µ–¥–Ω–∏–π –≤–∞—Ä–∏–∞–Ω—Ç –±–µ–∑ Unicode

#     # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
#     for line in report.split("\n"):
#         try:
#             if line.startswith("###"):
#                 pdf.set_font(size=14, style='B')
#                 pdf.cell(0, 10, line[3:].strip(), ln=True)
#                 pdf.set_font(size=12, style='')
#             else:
#                 pdf.multi_cell(0, 10, line)
#         except UnicodeEncodeError:
#             # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–µ—Ç—Å—è –∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å, –∑–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
#             cleaned_line = line.encode('utf-8', 'replace').decode('utf-8')
#             pdf.multi_cell(0, 10, cleaned_line)
    
#     path = f"/tmp/{file_name}"
#     pdf.output(path)
#     return path

# def get_binary_file_download_link(file_path, label="üì• –°–∫–∞—á–∞—Ç—å PDF"):
#     """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Å—ã–ª–∫–∏ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞"""
#     with open(file_path, 'rb') as f:
#         data = f.read()
#     b64 = base64.b64encode(data).decode()
#     return f'<a href="data:application/pdf;base64,{b64}" download="{os.path.basename(file_path)}">{label}</a>'

# def draw_similarity_chart(scores):
#     """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–æ–≤–µ—Ä–∫–∏"""
#     fig, ax = plt.subplots(figsize=(10, 4))
#     ax.bar(range(1, len(scores)+1), scores, color='#2e7d32')
#     ax.set_title("–ì—Ä–∞—Ñ–∏–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ —á–∞—Å—Ç—è–º —Ç–µ–∫—Å—Ç–∞", pad=20)
#     ax.set_xlabel("–ß–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞")
#     ax.set_ylabel("–ü—Ä–æ—Ü–µ–Ω—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è")
#     ax.set_ylim(0, 100)
#     st.pyplot(fig)

# # ===== –°—Ç–∏–ª–∏ =====
# st.markdown("""
#     <style>
#         .main-title { 
#             color: #2e7d32; 
#             font-family: 'Segoe UI'; 
#             font-size: 36px; 
#             font-weight: bold;
#             margin-bottom: 20px;
#         }
#         .stTabs [data-baseweb="tab"] { 
#             font-size: 18px; 
#             color: #2e7d32;
#             padding: 10px 20px;
#         }
#         .stButton>button {
#             background-color: #2e7d32;
#             color: white;
#             border: none;
#             border-radius: 6px;
#             padding: 0.5em 1em;
#             font-weight: bold;
#             margin: 5px 0;
#             transition: all 0.3s;
#         }
#         .stButton>button:hover { 
#             background-color: #388e3c; 
#             transform: scale(1.02);
#         }
#         .result-box {
#             background-color: #e8f5e9;
#             padding: 15px;
#             border-radius: 10px;
#             font-size: 16px;
#             line-height: 1.5;
#             margin: 15px 0;
#             border-left: 4px solid #2e7d32;
#         }
#         .file-info {
#             background-color: #f1f8e9;
#             padding: 10px;
#             border-radius: 5px;
#             margin: 10px 0;
#             border: 1px solid #c8e6c9;
#         }
#         .spinner-text {
#             color: #2e7d32;
#             font-weight: bold;
#             margin-top: 10px;
#         }
#         .error-box {
#             background-color: #ffebee;
#             padding: 15px;
#             border-radius: 10px;
#             border-left: 4px solid #f44336;
#             margin: 10px 0;
#         }
#     </style>
# """, unsafe_allow_html=True)

# # ===== –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å =====
# st.markdown('<div class="main-title">üß† AI Content Analyzer Pro</div>', unsafe_allow_html=True)

# tab1, tab2, tab3 = st.tabs(["üîç –ü—Ä–æ–≤–µ—Ä–∫–∞", "üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã", "üóÇ –ò—Å—Ç–æ—Ä–∏—è"])

# with tab1:
#     st.subheader("–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –ø—Ä–æ–≤–µ—Ä–∫–∏")
#     check_type = st.radio("", ["ü§ñ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ò–ò", "üìö –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–ª–∞–≥–∏–∞—Ç"], horizontal=True, label_visibility="collapsed")
    
#     if check_type == "ü§ñ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ò–ò":
#         st.markdown("""
#             <div style='margin: 15px 0; color: #3a3a3a;'>
#                 –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç, –±—ã–ª –ª–∏ –æ–Ω —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º.
#             </div>
#         """, unsafe_allow_html=True)
        
#         ai_file = st.file_uploader("üìÑ –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç (.txt, .pdf, .docx)", type=["txt", "pdf", "docx"], key="ai_file")
        
#         if st.button("üîé –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –ò–ò", key="ai_check"):
#             if ai_file:
#                 with st.spinner("–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç..."):
#                     try:
#                         text = extract_text(ai_file)
#                         if not text:
#                             st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞")
#                             st.stop()
                        
#                         chunks = split_text_into_chunks(text)
#                         results = []
                        
#                         progress_bar = st.progress(0)
#                         status_text = st.empty()
                        
#                         for i, chunk in enumerate(chunks):
#                             status_text.markdown(f'<div class="spinner-text">–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Å—Ç–∏ {i+1} –∏–∑ {len(chunks)}</div>', unsafe_allow_html=True)
#                             progress_bar.progress((i + 1) / len(chunks))
                            
#                             messages = [
#                                 {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é —Ç–µ–∫—Å—Ç–∞, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ò–ò. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –∏ –¥–∞–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏."},
#                                 {"role": "user", "content": f"–ë—ã–ª –ª–∏ —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –ò–ò? –û—Ç–≤–µ—Ç—å –ø–æ–¥—Ä–æ–±–Ω–æ —Å –∞–Ω–∞–ª–∏–∑–æ–º.\n\n{chunk}"}
#                             ]
                            
#                             result = safe_chat_completion(messages)
#                             if result:
#                                 results.append(f"### –ß–∞—Å—Ç—å {i+1}\n{result.choices[0].message.content}")
                        
#                         full_report = "\n\n".join(results)
#                         pdf_path = generate_pdf(full_report, f"ai_check_{ai_file.name}.pdf")
#                         save_to_db("AI Check", ai_file.name, full_report, [], pdf_path)
                        
#                         st.session_state["report"] = full_report
#                         st.session_state["pdf_path"] = pdf_path
#                         st.success("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω!")
#                     except Exception as e:
#                         st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {str(e)}")
#             else:
#                 st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
    
#     else:  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–ª–∞–≥–∏–∞—Ç
#         st.markdown("""
#             <div style='margin: 15px 0; color: #3a3a3a;'>
#                 –ó–∞–≥—Ä—É–∑–∏—Ç–µ —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —Ñ–∞–π–ª –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏. –°–∏—Å—Ç–µ–º–∞ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ –≤—ã–¥–∞—Å—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç.
#             </div>
#         """, unsafe_allow_html=True)
        
#         ref_files = st.file_uploader("üìÅ –≠—Ç–∞–ª–æ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (.txt, .pdf, .docx)", 
#                                    accept_multiple_files=True, 
#                                    type=["txt", "pdf", "docx"], 
#                                    key="ref_files")
        
#         target_file = st.file_uploader("üìÑ –î–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", 
#                                      type=["txt", "pdf", "docx"], 
#                                      key="plag_file")
        
#         if st.button("üîç –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ –ø–ª–∞–≥–∏–∞—Ç", key="plag_check"):
#             if ref_files and target_file:
#                 with st.spinner("–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è..."):
#                     try:
#                         ref_texts = [extract_text(f) for f in ref_files]
#                         target_text = extract_text(target_file)
                        
#                         if not target_text:
#                             st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞")
#                             st.stop()
                        
#                         target_chunks = split_text_into_chunks(target_text)
                        
#                         report = []
#                         similarity_scores = []
                        
#                         progress_bar = st.progress(0)
#                         status_text = st.empty()
                        
#                         for i, chunk in enumerate(target_chunks):
#                             status_text.markdown(f'<div class="spinner-text">–ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–∏ {i+1} –∏–∑ {len(target_chunks)}</div>', unsafe_allow_html=True)
#                             progress_bar.progress((i + 1) / len(target_chunks))
                            
#                             chunk_report = []
#                             max_similarity = 0
                            
#                             for j, ref in enumerate(ref_texts):
#                                 if not ref:
#                                     continue
                                
#                                 # –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
#                                 chunk_words = set(chunk.lower().split())
#                                 ref_words = set(ref.lower().split())
#                                 common_words = chunk_words & ref_words
                                
#                                 similarity = len(common_words) / max(len(chunk_words), 1) * 100
                                
#                                 if similarity > 15:  # –ü–æ—Ä–æ–≥ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
#                                     chunk_report.append(f"- –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º {j+1}: {similarity:.1f}%")
#                                     if similarity > max_similarity:
#                                         max_similarity = similarity
                            
#                             if chunk_report:
#                                 report.append(f"### –ß–∞—Å—Ç—å {i+1}\n" + "\n".join(chunk_report))
#                             else:
#                                 report.append(f"### –ß–∞—Å—Ç—å {i+1}\n- –°–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
                            
#                             similarity_scores.append(max_similarity)
                        
#                         full_report = "\n\n".join(report)
#                         pdf_path = generate_pdf(full_report, f"plagiarism_check_{target_file.name}.pdf")
#                         save_to_db("Plagiarism Check", target_file.name, full_report, similarity_scores, pdf_path)
                        
#                         st.session_state["report"] = full_report
#                         st.session_state["similarity"] = similarity_scores
#                         st.session_state["pdf_path"] = pdf_path
#                         st.success("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
#                     except Exception as e:
#                         st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –Ω–∞ –ø–ª–∞–≥–∏–∞—Ç: {str(e)}")
#             else:
#                 st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã.")

# with tab2:
#     if "report" in st.session_state:
#         st.markdown('<div class="result-box">{}</div>'.format(
#             st.session_state["report"].replace("\n", "<br>")), unsafe_allow_html=True)
        
#         if "similarity" in st.session_state and st.session_state["similarity"]:
#             st.markdown("---")
#             st.subheader("–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
#             draw_similarity_chart(st.session_state["similarity"])
        
#         st.markdown("---")
#         st.markdown(get_binary_file_download_link(
#             st.session_state["pdf_path"], 
#             "üì• –°–∫–∞—á–∞—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç –≤ PDF"
#         ), unsafe_allow_html=True)
#     else:
#         st.info("‚ÑπÔ∏è –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–æ–≤–µ—Ä–∫—É –≤–æ –≤–∫–ª–∞–¥–∫–µ 'üîç –ü—Ä–æ–≤–µ—Ä–∫–∞'.")

# with tab3:
#     st.subheader("–ò—Å—Ç–æ—Ä–∏—è –ø—Ä–æ–≤–µ—Ä–æ–∫")
#     history = load_history()
    
#     if history:
#         for row in history:
#             with st.expander(f"{row[2]} ‚Äî {row[1]} ({row[5]})"):
#                 st.markdown(f"**–¢–∏–ø –ø—Ä–æ–≤–µ—Ä–∫–∏:** {row[1]}")
#                 st.markdown(f"**–§–∞–π–ª:** {row[2]}")
#                 st.markdown(f"**–î–∞—Ç–∞:** {row[5]}")
                
#                 if st.button(f"–ü–æ–∫–∞–∑–∞—Ç—å –æ—Ç—á—ë—Ç #{row[0]}", key=f"show_{row[0]}"):
#                     st.markdown('<div class="result-box">{}</div>'.format(
#                         row[3].replace("\n", "<br>")), unsafe_allow_html=True)
                    
#                     if row[4]:
#                         try:
#                             scores = list(map(float, row[4].split(",")))
#                             draw_similarity_chart(scores)
#                         except:
#                             pass
                    
#                     st.markdown(get_binary_file_download_link(
#                         row[6], 
#                         f"üì• –°–∫–∞—á–∞—Ç—å –æ—Ç—á—ë—Ç #{row[0]}"
#                     ), unsafe_allow_html=True)
                
#                 st.markdown("---")
#     else:
#         st.info("üì≠ –ò—Å—Ç–æ—Ä–∏—è –ø—Ä–æ–≤–µ—Ä–æ–∫ –ø—É—Å—Ç–∞. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –Ω–∞—á–∞–ª–∞ –∞–Ω–∞–ª–∏–∑–∞.")


import streamlit as st
import os
import time
import random
import sqlite3
from datetime import datetime
from dotenv import load_dotenv
from openai import OpenAI, RateLimitError
import tiktoken
from fpdf import FPDF
import base64
import matplotlib.pyplot as plt
from PyPDF2 import PdfReader
from docx import Document as DocxDocument

# ===== –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î =====
def init_db():
    conn = sqlite3.connect("history.db")
    conn.execute("""
        CREATE TABLE IF NOT EXISTS checks (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            check_type TEXT,
            filename TEXT,
            report TEXT,
            similarity TEXT,
            timestamp TEXT,
            pdf_path TEXT
        )
    """)
    conn.commit()
    return conn

def save_to_db(check_type, filename, report, similarity_list, pdf_path):
    conn = init_db()
    similarity_str = ",".join(map(str, similarity_list)) if isinstance(similarity_list, list) else str(similarity_list)
    conn.execute(
        "INSERT INTO checks (check_type, filename, report, similarity, timestamp, pdf_path) VALUES (?, ?, ?, ?, ?, ?)",
        (check_type, filename, report, similarity_str, datetime.now().strftime("%Y-%m-%d %H:%M"), pdf_path)
    )
    conn.commit()
    conn.close()

def load_history():
    conn = init_db()
    rows = conn.execute("SELECT * FROM checks ORDER BY id DESC").fetchall()
    conn.close()
    return rows

# ===== API –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è =====
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    st.error("‚ùå API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")
    st.stop()
client = OpenAI(api_key=OPENAI_API_KEY)

# ===== –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ =====
def extract_text(file) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–æ–≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        if file.type == "application/pdf":
            return "".join(page.extract_text() or "" for page in PdfReader(file).pages)
        elif file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
            doc = DocxDocument(file)
            return "\n".join(p.text for p in doc.paragraphs)
        elif file.type == "text/plain":
            return file.read().decode("utf-8")
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {str(e)}")
        return ""
    return ""

def count_tokens(text: str, model="gpt-4") -> int:
    """–ü–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
    try:
        encoding = tiktoken.encoding_for_model(model)
        return len(encoding.encode(text))
    except:
        return len(text.split())  # Fallback –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

def split_text_into_chunks(text, max_tokens=2000, model="gpt-4"):
    """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç–æ–∫–µ–Ω–æ–≤"""
    encoding = tiktoken.encoding_for_model(model)
    tokens = encoding.encode(text)
    chunks = []
    
    for i in range(0, len(tokens), max_tokens):
        chunk_tokens = tokens[i:i+max_tokens]
        chunks.append(encoding.decode(chunk_tokens))
    
    return chunks

def safe_chat_completion(messages, model="gpt-4", max_tokens=1000, retries=3):
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ OpenAI —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π"""
    for attempt in range(retries):
        try:
            return client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=0.7
            )
        except RateLimitError:
            wait_time = 20 * (attempt + 1)
            st.warning(f"‚è≥ –õ–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤. –ñ–¥—ë–º {wait_time} —Å–µ–∫...")
            time.sleep(wait_time)
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ API: {str(e)}")
            break
    return None

def generate_pdf(report: str, file_name="report.pdf"):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è PDF —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Unicode"""
    pdf = FPDF()
    pdf.add_page()
    
    # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ Unicode (–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —à—Ä–∏—Ñ—Ç—ã)
    font_added = False
    current_font = "Arial"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    font_paths = [
        "DejaVuSansCondensed.ttf",
        "arial.ttf",
        "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",
        "C:/Windows/Fonts/arial.ttf"
    ]
    
    for font_path in font_paths:
        try:
            pdf.add_font('CustomFont', '', font_path, uni=True)
            current_font = 'CustomFont'
            font_added = True
            break
        except:
            continue
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —à—Ä–∏—Ñ—Ç
    pdf.set_font(current_font, size=12)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    for line in report.split("\n"):
        try:
            if line.startswith("###"):
                pdf.set_font(current_font, size=14, style='B')
                pdf.cell(0, 10, line[3:].strip(), ln=True)
                pdf.set_font(current_font, size=12, style='')
            else:
                pdf.multi_cell(0, 10, line)
        except UnicodeEncodeError:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–µ—Ç—Å—è –∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å, –∑–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
            cleaned_line = line.encode('utf-8', 'replace').decode('utf-8')
            pdf.multi_cell(0, 10, cleaned_line)
    
    path = f"/tmp/{file_name}"
    pdf.output(path)
    return path

def get_binary_file_download_link(file_path, label="üì• –°–∫–∞—á–∞—Ç—å PDF"):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Å—ã–ª–∫–∏ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞"""
    with open(file_path, 'rb') as f:
        data = f.read()
    b64 = base64.b64encode(data).decode()
    return f'<a href="data:application/pdf;base64,{b64}" download="{os.path.basename(file_path)}">{label}</a>'

def draw_similarity_chart(scores):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–æ–≤–µ—Ä–∫–∏"""
    fig, ax = plt.subplots(figsize=(10, 4))
    ax.bar(range(1, len(scores)+1), scores, color='#2e7d32')
    ax.set_title("–ì—Ä–∞—Ñ–∏–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ —á–∞—Å—Ç—è–º —Ç–µ–∫—Å—Ç–∞", pad=20)
    ax.set_xlabel("–ß–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞")
    ax.set_ylabel("–ü—Ä–æ—Ü–µ–Ω—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è")
    ax.set_ylim(0, 100)
    st.pyplot(fig)

# ===== –°—Ç–∏–ª–∏ =====
st.markdown("""
    <style>
        .main-title { 
            color: #2e7d32; 
            font-family: 'Segoe UI'; 
            font-size: 36px; 
            font-weight: bold;
            margin-bottom: 20px;
        }
        .stTabs [data-baseweb="tab"] { 
            font-size: 18px; 
            color: #2e7d32;
            padding: 10px 20px;
        }
        .stButton>button {
            background-color: #2e7d32;
            color: white;
            border: none;
            border-radius: 6px;
            padding: 0.5em 1em;
            font-weight: bold;
            margin: 5px 0;
            transition: all 0.3s;
        }
        .stButton>button:hover { 
            background-color: #388e3c; 
            transform: scale(1.02);
        }
        .result-box {
            background-color: #e8f5e9;
            padding: 15px;
            border-radius: 10px;
            font-size: 16px;
            line-height: 1.5;
            margin: 15px 0;
            border-left: 4px solid #2e7d32;
        }
        .file-info {
            background-color: #f1f8e9;
            padding: 10px;
            border-radius: 5px;
            margin: 10px 0;
            border: 1px solid #c8e6c9;
        }
        .spinner-text {
            color: #2e7d32;
            font-weight: bold;
            margin-top: 10px;
        }
        .error-box {
            background-color: #ffebee;
            padding: 15px;
            border-radius: 10px;
            border-left: 4px solid #f44336;
            margin: 10px 0;
        }
    </style>
""", unsafe_allow_html=True)

# ===== –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å =====
st.markdown('<div class="main-title">üß† AI Content Analyzer Pro</div>', unsafe_allow_html=True)

tab1, tab2, tab3 = st.tabs(["üîç –ü—Ä–æ–≤–µ—Ä–∫–∞", "üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã", "üóÇ –ò—Å—Ç–æ—Ä–∏—è"])

with tab1:
    st.subheader("–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –ø—Ä–æ–≤–µ—Ä–∫–∏")
    check_type = st.radio("", ["ü§ñ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ò–ò", "üìö –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–ª–∞–≥–∏–∞—Ç"], horizontal=True, label_visibility="collapsed")
    
    if check_type == "ü§ñ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ò–ò":
        st.markdown("""
            <div style='margin: 15px 0; color: #3a3a3a;'>
                –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç, –±—ã–ª –ª–∏ –æ–Ω —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º.
            </div>
        """, unsafe_allow_html=True)
        
        ai_file = st.file_uploader("üìÑ –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç (.txt, .pdf, .docx)", type=["txt", "pdf", "docx"], key="ai_file")
        
        if st.button("üîé –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –ò–ò", key="ai_check"):
            if ai_file:
                with st.spinner("–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç..."):
                    try:
                        text = extract_text(ai_file)
                        if not text:
                            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞")
                            st.stop()
                        
                        chunks = split_text_into_chunks(text)
                        results = []
                        
                        progress_bar = st.progress(0)
                        status_text = st.empty()
                        
                        for i, chunk in enumerate(chunks):
                            status_text.markdown(f'<div class="spinner-text">–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Å—Ç–∏ {i+1} –∏–∑ {len(chunks)}</div>', unsafe_allow_html=True)
                            progress_bar.progress((i + 1) / len(chunks))
                            
                            messages = [
                                {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é —Ç–µ–∫—Å—Ç–∞, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ò–ò. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –∏ –¥–∞–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏."},
                                {"role": "user", "content": f"–ë—ã–ª –ª–∏ —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –ò–ò? –û—Ç–≤–µ—Ç—å –ø–æ–¥—Ä–æ–±–Ω–æ —Å –∞–Ω–∞–ª–∏–∑–æ–º.\n\n{chunk}"}
                            ]
                            
                            result = safe_chat_completion(messages)
                            if result:
                                results.append(f"### –ß–∞—Å—Ç—å {i+1}\n{result.choices[0].message.content}")
                        
                        full_report = "\n\n".join(results)
                        pdf_path = generate_pdf(full_report, f"ai_check_{ai_file.name}.pdf")
                        save_to_db("AI Check", ai_file.name, full_report, [], pdf_path)
                        
                        st.session_state["report"] = full_report
                        st.session_state["pdf_path"] = pdf_path
                        st.success("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω!")
                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {str(e)}")
            else:
                st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
    
    else:  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–ª–∞–≥–∏–∞—Ç
        st.markdown("""
            <div style='margin: 15px 0; color: #3a3a3a;'>
                –ó–∞–≥—Ä—É–∑–∏—Ç–µ —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —Ñ–∞–π–ª –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏. –°–∏—Å—Ç–µ–º–∞ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ –≤—ã–¥–∞—Å—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç.
            </div>
        """, unsafe_allow_html=True)
        
        ref_files = st.file_uploader("üìÅ –≠—Ç–∞–ª–æ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (.txt, .pdf, .docx)", 
                                   accept_multiple_files=True, 
                                   type=["txt", "pdf", "docx"], 
                                   key="ref_files")
        
        target_file = st.file_uploader("üìÑ –î–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", 
                                     type=["txt", "pdf", "docx"], 
                                     key="plag_file")
        
        if st.button("üîç –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ –ø–ª–∞–≥–∏–∞—Ç", key="plag_check"):
            if ref_files and target_file:
                with st.spinner("–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è..."):
                    try:
                        ref_texts = [extract_text(f) for f in ref_files]
                        target_text = extract_text(target_file)
                        
                        if not target_text:
                            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞")
                            st.stop()
                        
                        target_chunks = split_text_into_chunks(target_text)
                        
                        report = []
                        similarity_scores = []
                        
                        progress_bar = st.progress(0)
                        status_text = st.empty()
                        
                        for i, chunk in enumerate(target_chunks):
                            status_text.markdown(f'<div class="spinner-text">–ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–∏ {i+1} –∏–∑ {len(target_chunks)}</div>', unsafe_allow_html=True)
                            progress_bar.progress((i + 1) / len(target_chunks))
                            
                            chunk_report = []
                            max_similarity = 0
                            
                            for j, ref in enumerate(ref_texts):
                                if not ref:
                                    continue
                                
                                # –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                                chunk_words = set(chunk.lower().split())
                                ref_words = set(ref.lower().split())
                                common_words = chunk_words & ref_words
                                
                                similarity = len(common_words) / max(len(chunk_words), 1) * 100
                                
                                if similarity > 15:  # –ü–æ—Ä–æ–≥ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                    chunk_report.append(f"- –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º {j+1}: {similarity:.1f}%")
                                    if similarity > max_similarity:
                                        max_similarity = similarity
                            
                            if chunk_report:
                                report.append(f"### –ß–∞—Å—Ç—å {i+1}\n" + "\n".join(chunk_report))
                            else:
                                report.append(f"### –ß–∞—Å—Ç—å {i+1}\n- –°–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
                            
                            similarity_scores.append(max_similarity)
                        
                        full_report = "\n\n".join(report)
                        pdf_path = generate_pdf(full_report, f"plagiarism_check_{target_file.name}.pdf")
                        save_to_db("Plagiarism Check", target_file.name, full_report, similarity_scores, pdf_path)
                        
                        st.session_state["report"] = full_report
                        st.session_state["similarity"] = similarity_scores
                        st.session_state["pdf_path"] = pdf_path
                        st.success("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –Ω–∞ –ø–ª–∞–≥–∏–∞—Ç: {str(e)}")
            else:
                st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã.")

with tab2:
    if "report" in st.session_state:
        st.markdown('<div class="result-box">{}</div>'.format(
            st.session_state["report"].replace("\n", "<br>")), unsafe_allow_html=True)
        
        if "similarity" in st.session_state and st.session_state["similarity"]:
            st.markdown("---")
            st.subheader("–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            draw_similarity_chart(st.session_state["similarity"])
        
        st.markdown("---")
        st.markdown(get_binary_file_download_link(
            st.session_state["pdf_path"], 
            "üì• –°–∫–∞—á–∞—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç –≤ PDF"
        ), unsafe_allow_html=True)
    else:
        st.info("‚ÑπÔ∏è –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–æ–≤–µ—Ä–∫—É –≤–æ –≤–∫–ª–∞–¥–∫–µ 'üîç –ü—Ä–æ–≤–µ—Ä–∫–∞'.")

with tab3:
    st.subheader("–ò—Å—Ç–æ—Ä–∏—è –ø—Ä–æ–≤–µ—Ä–æ–∫")
    history = load_history()
    
    if history:
        for row in history:
            with st.expander(f"{row[2]} ‚Äî {row[1]} ({row[5]})"):
                st.markdown(f"**–¢–∏–ø –ø—Ä–æ–≤–µ—Ä–∫–∏:** {row[1]}")
                st.markdown(f"**–§–∞–π–ª:** {row[2]}")
                st.markdown(f"**–î–∞—Ç–∞:** {row[5]}")
                
                if st.button(f"–ü–æ–∫–∞–∑–∞—Ç—å –æ—Ç—á—ë—Ç #{row[0]}", key=f"show_{row[0]}"):
                    st.markdown('<div class="result-box">{}</div>'.format(
                        row[3].replace("\n", "<br>")), unsafe_allow_html=True)
                    
                    if row[4]:
                        try:
                            scores = list(map(float, row[4].split(",")))
                            draw_similarity_chart(scores)
                        except:
                            pass
                    
                    st.markdown(get_binary_file_download_link(
                        row[6], 
                        f"üì• –°–∫–∞—á–∞—Ç—å –æ—Ç—á—ë—Ç #{row[0]}"
                    ), unsafe_allow_html=True)
                
                st.markdown("---")
    else:
        st.info("üì≠ –ò—Å—Ç–æ—Ä–∏—è –ø—Ä–æ–≤–µ—Ä–æ–∫ –ø—É—Å—Ç–∞. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –Ω–∞—á–∞–ª–∞ –∞–Ω–∞–ª–∏–∑–∞.")